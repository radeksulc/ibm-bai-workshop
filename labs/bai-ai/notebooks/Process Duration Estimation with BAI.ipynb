{"nbformat_minor": 1, "cells": [{"source": "", "cell_type": "markdown", "metadata": {}}, {"source": "# Using Spark ML and IBM Business Automation Insights to estimate process duration\n\nArtificial intelligence can be combined with business process management in more than one way. For example, Artificial intellignece can help transforming unstructured data into data that a process can work with, through techniques such as visual or text recognition. Assistants and bots can help providing a better user experience. Several IBM Watson services serve this purpose but a business process also captures a lot of business data.\nThis notebook shows how to take better benefit of this data and inject Machine Learning techniques to predict the duration of BPMN processes, based on the data captured by the process.\n\n## The  scenario\nThis notebook uses data that is stored from the BPMN process called 'Hiring Sample'. This process is a sample that is delivered with the Business Automation Workflow product. It represents the successive tasks to hire a new employee, from the submission of a new open position to the selection of the candidate, over the approval of the general manager. The purpose of this scenario is to reuse the data that is stored by this BPMN process, apply machine learning algorithms to predict the duration of a process, and possibly notify the submitter if the process is going to last long.\n\nThe purpose of this notebook is not to reproduce a real scenario but rather to show how such a scenario can be worked out by using Business Automation Insights.\n\n\n## Overview of the solution\n\nThis notebook assumes that that IBM Business Automation Workflow is used to run the business process and that this process management instance is connected to IBM Business Automation Insights so that all the operational data of the process is stored in an HDFS data lake.\n\n\n## Learning goals\n\nThe learning goals of this notebook are:\n\n- Understands 'time series' and 'completed summary' data from IBM Business Automation Insights\n- Explore the format of the data and learn how to read it\n- Create an Apache\u00ae Spark machine-learning pipeline to estimate process duration from existing data and business data\n- Train and evaluate the model.\n- Use the model to evaluate the duration of unfinished processes by leveraging the activity summary data of the process application.\n\n\n\n## Prerequisites\n\nMake sure that IBM Business Automation Workflow and IBM Business Automation Insights are installed.\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "## Creating some data to train the system\n\nIf you want to exercise with the notebook, you will have to create several instances of the Hiring Sample process and complete part of the process. The completed parts are then used to estimate the process duration. Leave some processes unfinished so that the duration of the unfinished part can be computed.", "cell_type": "markdown", "metadata": {}}, {"source": "## The format of the Business Automation Insight data\nThis exercice uses both the BPMN 'time series' events and the BPMN 'summaries' events. Time series events represent raw events that are captured as activities or processes that are executed. The 'summaries' events are stored to HDFS when a process or activity is completed and contain information such as the duration of the process or activity. All data are stored in the JSON format. The paths are defined as follows:\n\n * Process timeseries:\n\n    [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/process/[processId]/[date]\n\n\n * Activity timeseries:\n\n    [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/activity/[processId]/[activityId]/[date]\n\n\n * Process summary paths:\n\n    [path_to_your_hdfs]/ibm-bai/bpmn-summaries-completed/[process Application Id]/[process Application Version Id]/process/[process Id]/[date]\n    \n\n * Activity summary paths: \n \n    [path_to_your_hdfs]/ibm-bai/bpmn-summaries-completed/[process Application Id]/[process Application Version Id]/activity/[process Id]/[activity Id]/[date]", "cell_type": "markdown", "metadata": {}}, {"source": "## Using Spark SQL to read Business Automation Insights data\nBusiness Automation Insight stores data in HDFS. As indicated above, events coming from the Business Automation Workflow instance are stored in JSON files. The data is read through Spark SQL. ", "cell_type": "markdown", "metadata": {}}, {"source": "This first query reads the data from all completed process instances in the 'Hiring Sample' process. Here the identifier of the process application is hard-coded because the identifier for the Hiring Sample is the same across various Business Automation Workflow installations. If you need to adapt the code to your particular process application, find the process application identifier and the process application version identifier. Page https://www.ibm.com/support/knowledgecenter/SSYHZ8_18.0.0/com.ibm.dba.bai/topics/tsk_bai_retrieve_process_info_from_id.html of the Business Automation Insights documentation explains how to find those identifiers by using the REST API. In the code below, make sure you <strong>change the HDFS URL</strong>.\n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql import SparkSession, SQLContext, Row\nfrom pyspark.sql.types import IntegerType\nfrom datetime import datetime\n\nprocessAppId = '9ab0d0c6-d92c-4355-9ed5-d8a05acdc4b0'\nprocessAppVersionId = '2064.64cc52a5-fdc6-4cab-8e36-3ded94921f56'\n\nhdfsroot = 'hdfs://path to your hdfs'\n\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set(\"dfs.client.use.datanode.hostname\", \"true\")\n\n\nprocessSummaries = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-summaries-completed/\"+processAppId+\"/\"+processAppVersionId+\"/process/*/*\")\nprocessSummaries.createOrReplaceTempView(\"processSummaries\")\nprint ('The schema of the completed processes')\n\nprocessSummaries.printSchema()\nprint ('The data containts ' + str(processSummaries.count()) + ' events')\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "You should get a schema for events that looks like:\n    <pre>\nroot\n |-- bpmCellName: string (nullable = true)\n |-- bpmSystemId: string (nullable = true)\n |-- completedTime: string (nullable = true)\n |-- data: struct (nullable = true)\n |    |-- aEmpRequisition121381434563922: struct (nullable = true)\n |    |    |-- @ids: struct (nullable = true)\n |    |    |    |-- trackingGroupId: string (nullable = true)\n |    |    |    |-- trackingGroupVersionId: string (nullable = true)\n |    |    |-- Department.string: string (nullable = true)\n |    |    |-- EmploymentStatus.string: string (nullable = true)\n |    |    |-- GMApproval.string: string (nullable = true)\n |    |    |-- HiringManager.string: string (nullable = true)\n |    |    |-- Location.string: string (nullable = true)\n |-- deletedTime: string (nullable = true)\n |-- duration: long (nullable = true)\n |-- id: string (nullable = true)\n |-- lastBusinessDataUpdateActivity: string (nullable = true)\n |-- lastBusinessDataUpdateEvent: string (nullable = true)\n |-- lastBusinessDataUpdatePerformer: string (nullable = true)\n |-- lastBusinessDataUpdateTime: string (nullable = true)\n |-- name: string (nullable = true)\n |-- processApplicationId: string (nullable = true)\n |-- processApplicationName: string (nullable = true)\n |-- processApplicationSnapshotName: string (nullable = true)\n |-- processApplicationVersionId: string (nullable = true)\n |-- processId: string (nullable = true)\n |-- processSnapshotName: string (nullable = true)\n |-- processVersionId: string (nullable = true)\n |-- startTime: string (nullable = true)\n |-- state: string (nullable = true)\n |-- terminatedTime: string (nullable = true)\n |-- timestamp: string (nullable = true)\n |-- type: string (nullable = true)\n |-- version: string (nullable = true)\n    </pre>", "cell_type": "markdown", "metadata": {}}, {"source": "In the process summary event, you are interested in the overall process duration that is visible in the 'duration' column. You will also use the 'startTime' of the process and the business data that is attached to the Business Automation Workflow process. The process stores some business data related to the hiring process, such as the hiring department, the type of employment, the hiring location, and the manager. This data, the process start time, and the duration are used to build a machine-learning model that can predict the duration of processes. Real cases involve more parameters but this simplified example is sufficient for the demonstration.\n\nThe hiring process has the 'autotracking' flag on, which means that all activity events and process events will embed the business data. The business data is tracked through a tracking group, whose autogenerated name is: aEmpRequisition121381434563922. The event structure shows that all the business data is stored under the data.aEmpRequisition121381434563922 object.", "cell_type": "markdown", "metadata": {}}, {"source": "processsummaries = spark.sql(\"SELECT  duration, startTime, data.aEmpRequisition121381434563922.* FROM processsummaries\");\n\nprocesssummaries.show()\nprocesssummaries.printSchema()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "The resulting schema for the processSummary data set is: \n    <pre>\nroot\n |-- duration: long (nullable = true)\n |-- startTime: string (nullable = true)\n |-- @ids: struct (nullable = true)\n |    |-- trackingGroupId: string (nullable = true)\n |    |-- trackingGroupVersionId: string (nullable = true)\n |-- Department.string: string (nullable = true)\n |-- EmploymentStatus.string: string (nullable = true)\n |-- GMApproval.string: string (nullable = true)\n |-- HiringManager.string: string (nullable = true)\n |-- Location.string: string (nullable = true)\n    </pre>", "cell_type": "markdown", "metadata": {}}, {"source": "## Create an Apache\u00ae Spark machine learning model\n\nWatson Machine learning supports a growing number of IBM or open-source machine-learning and deep-learning packages. This example uses Spark ML, in particular the Linear Regression algorithm. You are now going to learn how to prepare data, create an Apache\u00c6 Spark machine-learning pipeline, and train the model.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline, Model", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Adaptation of data\n\nThe following code renames the columns to remove their type.\nThen, several StringIndexer are created to transform string-typed columns (which represent categories) into numeric values. The VectorAssembler class creates a new 'features' column, which contains the features from which you will build the model.\n\nAn SQL Transformer query transforms the startTime column into three numeric columns (year, month, and dayofMonth).", "cell_type": "markdown", "metadata": {}}, {"source": "processsummaries = processsummaries.withColumnRenamed(\"Department.string\", \"Department\")\nprocesssummaries = processsummaries.withColumnRenamed(\"EmploymentStatus.string\", \"EmploymentStatus\")\nprocesssummaries = processsummaries.withColumnRenamed(\"HiringManager.string\", \"HiringManager\")\nprocesssummaries = processsummaries.withColumnRenamed(\"Location.string\", \"Location\")\n\nprocesssummaries.show()\n\ndepartmentIndexer = StringIndexer(inputCol='Department', outputCol=\"IndexedDepartment\").setHandleInvalid(\"skip\").fit(processsummaries)\nemploymentStatusIndexer = StringIndexer(inputCol='EmploymentStatus', outputCol=\"IndexedEmploymentStatus\").fit(processsummaries)\nhiringManagerIndexer = StringIndexer(inputCol='HiringManager', outputCol=\"IndexedHiringManager\").setHandleInvalid(\"skip\").fit(processsummaries)\nlocationIndexer = StringIndexer(inputCol='Location', outputCol=\"IndexedLocation\").setHandleInvalid(\"skip\").fit(processsummaries)\n\nfrom pyspark.ml.feature import SQLTransformer\n\ndateTransformer = SQLTransformer(\n    statement=\"SELECT *, year(startTime) AS year, dayOfMonth(startTime) as dayOfMonth, month(startTime) as month FROM __THIS__\")\n\n\nfeatures = [\"IndexedDepartment\", \"IndexedEmploymentStatus\", \"IndexedHiringManager\", \"IndexedLocation\", \"year\", \"month\", \"dayOfMonth\"]\n\nassembler = VectorAssembler(inputCols=features, outputCol=\"features\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "###  Creating the model\nThe model is built through the Linear Regression algorithm.", "cell_type": "markdown", "metadata": {}}, {"source": "lr = LinearRegression(labelCol=\"duration\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "In the cell below, the data is split into training data and test data, and the prediction model is trained and then tested. Finally, the accuracy of the model is displayed as the root mean square error.", "cell_type": "markdown", "metadata": {}}, {"source": "\nsplitted_data = processsummaries.randomSplit([0.8, 0.20], 24)\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\n\npipeline = Pipeline(stages=[departmentIndexer, employmentStatusIndexer, \n                            hiringManagerIndexer, locationIndexer, dateTransformer, assembler, lr ])\n\nmodel = pipeline.fit(train_data)\n\npredictions = model.transform(test_data)\n\nevaluator = RegressionEvaluator(labelCol=\"duration\")\n\nrmse = evaluator.evaluate(predictions)\n\ndef toHMS(duration):\n    duration = duration // 1000\n    h = int(duration // 3600)\n    m = int ((duration % 3600) // 60)\n    s = int((duration % 3600) % 60)\n    return str(h) +\"H\" + str(m) + \"M\" +  str(s) + \"s\"\n\nprint(\"Root Mean Square Error = \" + toHMS(rmse))\n\nfrom pyspark.sql.functions import col, expr, when, max, min, avg\nmin_max = processsummaries.agg(min(\"duration\"), max(\"duration\"), avg(\"duration\")).head()\n\nmin_duration = min_max[0]\nmax_duration = min_max[1]\navg_duration = min_max[2]\n\n\nprint(\"Min duration = \" + toHMS(min_duration))\nprint(\"Max duration = \" + toHMS(max_duration))\nprint(\"Avg duration = \" + toHMS(avg_duration))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## Evaluation of the duration of noncompleted processes", "cell_type": "markdown", "metadata": {}}, {"source": "The sample then uses the model to evaluate the duration of noncompleted processes. First, the list of noncompleted processes is computed. The Business Automation Insights process time series events are used for this purpose. A SPARK SQL join query between PROCESS_STARTED and PROCESS_COMPLETED events finds the process instances that are started but not completed.\n\nThe time series events for the process application are located on this path: [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/process/[process Id]/[date]\n\nThe \"type\" property of the events is used to distinguish between PROCESS_STARTED and PROCESS_COMPLETED events. A join between the two tables returns the list of processes that are not completed.", "cell_type": "markdown", "metadata": {}}, {"source": "processTimeseries = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-timeseries/\"+processAppId+\"/\"+processAppVersionId+\"/process/*/*\")\n\nprocessTimeseries.createOrReplaceTempView(\"processTimeseries\")\n\ncompletedProcesses = spark.sql(\"SELECT  * FROM processTimeseries WHERE type = 'PROCESS_COMPLETED' \")\ncompletedProcesses.createOrReplaceTempView(\"completed\")\ncompletedProcessesCount = completedProcesses.count();\n\n\nstartedProcesses = spark.sql(\"SELECT  * FROM processTimeseries WHERE type = 'PROCESS_STARTED' \")\nstartedProcesses.createOrReplaceTempView(\"started\")\nstartedProcessesCount = startedProcesses.count();\n\n#This Left outer join will select the processes that are in the started list but not in the completed list.\n\nunfinishedProcesses = spark.sql(\"SELECT distinct s.processInstanceId, s.timeStamp as startTime FROM  started s LEFT OUTER JOIN completed c ON s.processInstanceId=c.processInstanceId WHERE c.processInstanceId is NULL\")\nunfinishedProcesses.createOrReplaceTempView(\"unfinishedProcesses\")\n\nprint (\"There are \" + str(unfinishedProcesses.count()) + \" non completed processes over \" + str(completedProcessesCount + startedProcessesCount)  +\" total process instances\" )\nunfinishedProcesses.printSchema()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Searching for activities and tracked data of uncompleted processes\n\nNow that the list of uncompleted process instances is computed, the next step is to evaluate their duration. For this evaluation, you need the business data that is associated with each process instance. Some processes might be started but with no activity completed, so some process instances might have no business data available. In the Hiring sample case, for the duration of the process to be evaluated, the work position must be published. If we look at the list of completed activities by using the BAI summary events for activities, because autotracking is set to 'on', the events carry the business data.\n\nThe code below operates a left outer join to list, within all the completed activity events, those that are part of an uncompleted process instance.", "cell_type": "markdown", "metadata": {}}, {"source": "activities = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-summaries-completed/\"+processAppId+\"/\"+processAppVersionId+\"/activity/*/*/*\").createOrReplaceTempView(\"activities\")\n\nactivitiesOfUnfinishedProcesses = spark.sql(\"select p.startTime, a.name, a.timeStamp, a.data.aEmpRequisition121381434563922.* , p.processInstanceId from activities a LEFT OUTER JOIN unfinishedProcesses p ON a.processInstanceId=p.processInstanceId where p.processInstanceId is not NULL\")\nactivitiesOfUnfinishedProcesses.show()\n\nactivitiesOfUnfinishedProcesses.createOrReplaceTempView(\"activitiesOfUnfinishedProcesses\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "You now want to find the last activity of the unfinished processes. This is done by partitioning the events by the process instance identifier, then ordering them by the timestamp, and taking the first event. The result consists of the last activity event that was stored for each uncompleted process.", "cell_type": "markdown", "metadata": {}}, {"source": "unfinishedProcesses = spark.sql(\"SELECT distinct * FROM ( SELECT *, dense_rank() OVER (PARTITION BY processInstanceId ORDER BY timeStamp DESC) AS rank  FROM activitiesOfUnfinishedProcesses) vo WHERE rank = 1\");\nunfinishedProcesses.show()\nunfinishedProcesses.createOrReplaceTempView(\"unfinishedProcesses\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "The ML model can now be used to predict the duration of the uncompleted processes:", "cell_type": "markdown", "metadata": {}}, {"source": "unfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"Department.string\", \"Department\")\nunfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"HiringManager.string\", \"HiringManager\")\nunfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"Location.string\", \"Location\")\nunfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"EmploymentStatus.string\", \"EmploymentStatus\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model.transform(unfinishedProcesses).select(\"prediction\").show()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Conclusion\nYou have seen the various type of events that are stored in Business Automation Insights for activities and processes, and how you can take benefit of them to predict the process duration, based on the business data tracked by a process.", "cell_type": "markdown", "metadata": {}}, {"source": "Author: Emmanuel Tissandier is a Senior Technical Staff Member and architect in the Digital Business Automation team in the IBM France Lab.", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}