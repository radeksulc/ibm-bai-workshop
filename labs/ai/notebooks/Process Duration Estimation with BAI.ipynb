{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Spark ML and IBM Business Automation Insights to estimate process duration\n",
    "\n",
    "Artificial intelligence can be combined with business process management in more than one way. For example, Artificial intellignece can help transforming unstructured data into data that a process can work with, through techniques such as visual or text recognition. Assistants and bots can help providing a better user experience. Several IBM Watson services serve this purpose but a business process also captures a lot of business data.\n",
    "This notebook shows how to take better benefit of this data and inject Machine Learning techniques to predict the duration of BPMN processes, based on the data captured by the process.\n",
    "\n",
    "## The  scenario\n",
    "This notebook uses data that is stored from the BPMN process called 'Hiring Sample'. This process is a sample that is delivered with the Business Automation Workflow product. It represents the successive tasks to hire a new employee, from the submission of a new open position to the selection of the candidate, over the approval of the general manager. The purpose of this scenario is to reuse the data that is stored by this BPMN process, apply machine learning algorithms to predict the duration of a process, and possibly notify the submitter if the process is going to last long.\n",
    "\n",
    "The purpose of this notebook is not to reproduce a real scenario but rather to show how such a scenario can be worked out by using Business Automation Insights.\n",
    "\n",
    "\n",
    "## Overview of the solution\n",
    "\n",
    "This notebook assumes that that IBM Business Automation Workflow is used to run the business process and that this process management instance is connected to IBM Business Automation Insights so that all the operational data of the process is stored in an HDFS data lake.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "- Understands 'time series' and 'completed summary' data from IBM Business Automation Insights\n",
    "- Explore the format of the data and learn how to read it\n",
    "- Create an Apache® Spark machine-learning pipeline to estimate process duration from existing data and business data\n",
    "- Train and evaluate the model.\n",
    "- Use the model to evaluate the duration of unfinished processes by leveraging the activity summary data of the process application.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure that IBM Business Automation Workflow and IBM Business Automation Insights are installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some data to train the system\n",
    "\n",
    "If you want to exercise with the notebook, you will have to create several instances of the Hiring Sample process and complete part of the process. The completed parts are then used to estimate the process duration. Leave some processes unfinished so that the duration of the unfinished part can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The format of the Business Automation Insight data\n",
    "This exercice uses both the BPMN 'time series' events and the BPMN 'summaries' events. Time series events represent raw events that are captured as activities or processes that are executed. The 'summaries' events are stored to HDFS when a process or activity is completed and contain information such as the duration of the process or activity. All data are stored in the JSON format. The paths are defined as follows:\n",
    "\n",
    " * Process timeseries:\n",
    "\n",
    "    [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/process/[processId]/[date]\n",
    "\n",
    "\n",
    " * Activity timeseries:\n",
    "\n",
    "    [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/activity/[processId]/[activityId]/[date]\n",
    "\n",
    "\n",
    " * Process summary paths:\n",
    "\n",
    "    [path_to_your_hdfs]/ibm-bai/bpmn-summaries-completed/[process Application Id]/[process Application Version Id]/process/[process Id]/[date]\n",
    "    \n",
    "\n",
    " * Activity summary paths: \n",
    " \n",
    "    [path_to_your_hdfs]/ibm-bai/bpmn-summaries-completed/[process Application Id]/[process Application Version Id]/activity/[process Id]/[activity Id]/[date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL to read Business Automation Insights data\n",
    "Business Automation Insight stores data in HDFS. As indicated above, events coming from the Business Automation Workflow instance are stored in JSON files. The data is read through Spark SQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first query reads the data from all completed process instances in the 'Hiring Sample' process. Here the identifier of the process application is hard-coded because the identifier for the Hiring Sample is the same across various Business Automation Workflow installations. If you need to adapt the code to your particular process application, find the process application identifier and the process application version identifier. Page https://www.ibm.com/support/knowledgecenter/SSYHZ8_18.0.0/com.ibm.dba.bai/topics/tsk_bai_retrieve_process_info_from_id.html of the Business Automation Insights documentation explains how to find those identifiers by using the REST API. In the code below, make sure you <strong>change the HDFS URL</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema of the completed processes\n",
      "root\n",
      " |-- bpmCellName: string (nullable = true)\n",
      " |-- bpmSystemId: string (nullable = true)\n",
      " |-- completedTime: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- TG_MA: struct (nullable = true)\n",
      " |    |    |-- @ids: struct (nullable = true)\n",
      " |    |    |    |-- trackingGroupId: string (nullable = true)\n",
      " |    |    |    |-- trackingGroupVersionId: string (nullable = true)\n",
      " |    |    |-- country.string: string (nullable = true)\n",
      " |    |    |-- customerName.string: string (nullable = true)\n",
      " |    |    |-- dateOfBirth.dateTime: string (nullable = true)\n",
      " |    |    |-- finalReview.string: string (nullable = true)\n",
      " |    |    |-- landRegistrarEvaluation.string: string (nullable = true)\n",
      " |    |    |-- landRegistrarMessage.string: string (nullable = true)\n",
      " |    |    |-- loanAmount.integer: long (nullable = true)\n",
      " |    |    |-- netIncome.integer: long (nullable = true)\n",
      " |    |    |-- propertyAddress.string: string (nullable = true)\n",
      " |    |    |-- purchasingPrice.integer: long (nullable = true)\n",
      " |    |    |-- riskAssessmentDecision.string: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- lastBusinessDataUpdateActivity: string (nullable = true)\n",
      " |-- lastBusinessDataUpdateEvent: string (nullable = true)\n",
      " |-- lastBusinessDataUpdatePerformer: string (nullable = true)\n",
      " |-- lastBusinessDataUpdateTime: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent-span-id: string (nullable = true)\n",
      " |-- processApplicationId: string (nullable = true)\n",
      " |-- processApplicationName: string (nullable = true)\n",
      " |-- processApplicationVersionId: string (nullable = true)\n",
      " |-- processId: string (nullable = true)\n",
      " |-- processInstanceId: string (nullable = true)\n",
      " |-- processVersionId: string (nullable = true)\n",
      " |-- shortProcessInstanceId: string (nullable = true)\n",
      " |-- span-id: string (nullable = true)\n",
      " |-- startTime: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- trace-id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n",
      "The data containts 11 events\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "processAppId = '2dc9934a-288f-4a56-b370-8d2c8121ce06'\n",
    "processAppVersionId = '2064.2948e817-5d9f-4e0d-b951-b531800242f8'\n",
    "\n",
    "hdfsroot = 'hdfs://mini/user/bai'\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"dfs.client.use.datanode.hostname\", \"true\")\n",
    "\n",
    "\n",
    "processSummaries = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-summaries-completed/\"+processAppId+\"/\"+processAppVersionId+\"/process/*/*\")\n",
    "processSummaries.createOrReplaceTempView(\"processSummaries\")\n",
    "print ('The schema of the completed processes')\n",
    "\n",
    "processSummaries.printSchema()\n",
    "print ('The data containts ' + str(processSummaries.count()) + ' events')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a schema for events that looks like:\n",
    "    <pre>\n",
    "root\n",
    " |-- bpmCellName: string (nullable = true)\n",
    " |-- bpmSystemId: string (nullable = true)\n",
    " |-- completedTime: string (nullable = true)\n",
    " |-- data: struct (nullable = true)\n",
    " |    |-- aEmpRequisition121381434563922: struct (nullable = true)\n",
    " |    |    |-- @ids: struct (nullable = true)\n",
    " |    |    |    |-- trackingGroupId: string (nullable = true)\n",
    " |    |    |    |-- trackingGroupVersionId: string (nullable = true)\n",
    " |    |    |-- Department.string: string (nullable = true)\n",
    " |    |    |-- EmploymentStatus.string: string (nullable = true)\n",
    " |    |    |-- GMApproval.string: string (nullable = true)\n",
    " |    |    |-- HiringManager.string: string (nullable = true)\n",
    " |    |    |-- Location.string: string (nullable = true)\n",
    " |-- deletedTime: string (nullable = true)\n",
    " |-- duration: long (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- lastBusinessDataUpdateActivity: string (nullable = true)\n",
    " |-- lastBusinessDataUpdateEvent: string (nullable = true)\n",
    " |-- lastBusinessDataUpdatePerformer: string (nullable = true)\n",
    " |-- lastBusinessDataUpdateTime: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- processApplicationId: string (nullable = true)\n",
    " |-- processApplicationName: string (nullable = true)\n",
    " |-- processApplicationSnapshotName: string (nullable = true)\n",
    " |-- processApplicationVersionId: string (nullable = true)\n",
    " |-- processId: string (nullable = true)\n",
    " |-- processSnapshotName: string (nullable = true)\n",
    " |-- processVersionId: string (nullable = true)\n",
    " |-- startTime: string (nullable = true)\n",
    " |-- state: string (nullable = true)\n",
    " |-- terminatedTime: string (nullable = true)\n",
    " |-- timestamp: string (nullable = true)\n",
    " |-- type: string (nullable = true)\n",
    " |-- version: string (nullable = true)\n",
    "    </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process summary event, you are interested in the overall process duration that is visible in the 'duration' column. You will also use the 'startTime' of the process and the business data that is attached to the Business Automation Workflow process. The process stores some business data related to the hiring process, such as the hiring department, the type of employment, the hiring location, and the manager. This data, the process start time, and the duration are used to build a machine-learning model that can predict the duration of processes. Real cases involve more parameters but this simplified example is sufficient for the demonstration.\n",
    "\n",
    "The hiring process has the 'autotracking' flag on, which means that all activity events and process events will embed the business data. The business data is tracked through a tracking group, whose autogenerated name is: aEmpRequisition121381434563922. The event structure shows that all the business data is stored under the data.aEmpRequisition121381434563922 object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "No such struct field aEmpRequisition121381434563922 in TG_MA;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d5d32d1c4fe9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocesssummaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT  duration, startTime, data.aEmpRequisition121381434563922.* FROM processsummaries\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprocesssummaries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprocesssummaries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: No such struct field aEmpRequisition121381434563922 in TG_MA;"
     ]
    }
   ],
   "source": [
    "processsummaries = spark.sql(\"SELECT  duration, startTime, data.aEmpRequisition121381434563922.* FROM processsummaries\");\n",
    "\n",
    "processsummaries.show()\n",
    "processsummaries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting schema for the processSummary data set is: \n",
    "    <pre>\n",
    "root\n",
    " |-- duration: long (nullable = true)\n",
    " |-- startTime: string (nullable = true)\n",
    " |-- @ids: struct (nullable = true)\n",
    " |    |-- trackingGroupId: string (nullable = true)\n",
    " |    |-- trackingGroupVersionId: string (nullable = true)\n",
    " |-- Department.string: string (nullable = true)\n",
    " |-- EmploymentStatus.string: string (nullable = true)\n",
    " |-- GMApproval.string: string (nullable = true)\n",
    " |-- HiringManager.string: string (nullable = true)\n",
    " |-- Location.string: string (nullable = true)\n",
    "    </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Apache® Spark machine learning model\n",
    "\n",
    "Watson Machine learning supports a growing number of IBM or open-source machine-learning and deep-learning packages. This example uses Spark ML, in particular the Linear Regression algorithm. You are now going to learn how to prepare data, create an ApacheÆ Spark machine-learning pipeline, and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation of data\n",
    "\n",
    "The following code renames the columns to remove their type.\n",
    "Then, several StringIndexer are created to transform string-typed columns (which represent categories) into numeric values. The VectorAssembler class creates a new 'features' column, which contains the features from which you will build the model.\n",
    "\n",
    "An SQL Transformer query transforms the startTime column into three numeric columns (year, month, and dayofMonth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processsummaries = processsummaries.withColumnRenamed(\"Department.string\", \"Department\")\n",
    "processsummaries = processsummaries.withColumnRenamed(\"EmploymentStatus.string\", \"EmploymentStatus\")\n",
    "processsummaries = processsummaries.withColumnRenamed(\"HiringManager.string\", \"HiringManager\")\n",
    "processsummaries = processsummaries.withColumnRenamed(\"Location.string\", \"Location\")\n",
    "\n",
    "processsummaries.show()\n",
    "\n",
    "departmentIndexer = StringIndexer(inputCol='Department', outputCol=\"IndexedDepartment\").setHandleInvalid(\"skip\").fit(processsummaries)\n",
    "employmentStatusIndexer = StringIndexer(inputCol='EmploymentStatus', outputCol=\"IndexedEmploymentStatus\").fit(processsummaries)\n",
    "hiringManagerIndexer = StringIndexer(inputCol='HiringManager', outputCol=\"IndexedHiringManager\").setHandleInvalid(\"skip\").fit(processsummaries)\n",
    "locationIndexer = StringIndexer(inputCol='Location', outputCol=\"IndexedLocation\").setHandleInvalid(\"skip\").fit(processsummaries)\n",
    "\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "dateTransformer = SQLTransformer(\n",
    "    statement=\"SELECT *, year(startTime) AS year, dayOfMonth(startTime) as dayOfMonth, month(startTime) as month FROM __THIS__\")\n",
    "\n",
    "\n",
    "features = [\"IndexedDepartment\", \"IndexedEmploymentStatus\", \"IndexedHiringManager\", \"IndexedLocation\", \"year\", \"month\", \"dayOfMonth\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating the model\n",
    "The model is built through the Linear Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol=\"duration\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, the data is split into training data and test data, and the prediction model is trained and then tested. Finally, the accuracy of the model is displayed as the root mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitted_data = processsummaries.randomSplit([0.8, 0.20], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "pipeline = Pipeline(stages=[departmentIndexer, employmentStatusIndexer, \n",
    "                            hiringManagerIndexer, locationIndexer, dateTransformer, assembler, lr ])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"duration\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "def toHMS(duration):\n",
    "    duration = duration // 1000\n",
    "    h = int(duration // 3600)\n",
    "    m = int ((duration % 3600) // 60)\n",
    "    s = int((duration % 3600) % 60)\n",
    "    return str(h) +\"H\" + str(m) + \"M\" +  str(s) + \"s\"\n",
    "\n",
    "print(\"Root Mean Square Error = \" + toHMS(rmse))\n",
    "\n",
    "from pyspark.sql.functions import col, expr, when, max, min, avg\n",
    "min_max = processsummaries.agg(min(\"duration\"), max(\"duration\"), avg(\"duration\")).head()\n",
    "\n",
    "min_duration = min_max[0]\n",
    "max_duration = min_max[1]\n",
    "avg_duration = min_max[2]\n",
    "\n",
    "\n",
    "print(\"Min duration = \" + toHMS(min_duration))\n",
    "print(\"Max duration = \" + toHMS(max_duration))\n",
    "print(\"Avg duration = \" + toHMS(avg_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the duration of noncompleted processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample then uses the model to evaluate the duration of noncompleted processes. First, the list of noncompleted processes is computed. The Business Automation Insights process time series events are used for this purpose. A SPARK SQL join query between PROCESS_STARTED and PROCESS_COMPLETED events finds the process instances that are started but not completed.\n",
    "\n",
    "The time series events for the process application are located on this path: [path_to_your_hdfs]/ibm-bai/bpmn-timeseries/[process Application Id]/[process Application Version Id]/process/[process Id]/[date]\n",
    "\n",
    "The \"type\" property of the events is used to distinguish between PROCESS_STARTED and PROCESS_COMPLETED events. A join between the two tables returns the list of processes that are not completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTimeseries = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-timeseries/\"+processAppId+\"/\"+processAppVersionId+\"/process/*/*\")\n",
    "\n",
    "processTimeseries.createOrReplaceTempView(\"processTimeseries\")\n",
    "\n",
    "completedProcesses = spark.sql(\"SELECT  * FROM processTimeseries WHERE type = 'PROCESS_COMPLETED' \")\n",
    "completedProcesses.createOrReplaceTempView(\"completed\")\n",
    "completedProcessesCount = completedProcesses.count();\n",
    "\n",
    "\n",
    "startedProcesses = spark.sql(\"SELECT  * FROM processTimeseries WHERE type = 'PROCESS_STARTED' \")\n",
    "startedProcesses.createOrReplaceTempView(\"started\")\n",
    "startedProcessesCount = startedProcesses.count();\n",
    "\n",
    "#This Left outer join will select the processes that are in the started list but not in the completed list.\n",
    "\n",
    "unfinishedProcesses = spark.sql(\"SELECT distinct s.processInstanceId, s.timeStamp as startTime FROM  started s LEFT OUTER JOIN completed c ON s.processInstanceId=c.processInstanceId WHERE c.processInstanceId is NULL\")\n",
    "unfinishedProcesses.createOrReplaceTempView(\"unfinishedProcesses\")\n",
    "\n",
    "print (\"There are \" + str(unfinishedProcesses.count()) + \" non completed processes over \" + str(completedProcessesCount + startedProcessesCount)  +\" total process instances\" )\n",
    "unfinishedProcesses.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for activities and tracked data of uncompleted processes\n",
    "\n",
    "Now that the list of uncompleted process instances is computed, the next step is to evaluate their duration. For this evaluation, you need the business data that is associated with each process instance. Some processes might be started but with no activity completed, so some process instances might have no business data available. In the Hiring sample case, for the duration of the process to be evaluated, the work position must be published. If we look at the list of completed activities by using the BAI summary events for activities, because autotracking is set to 'on', the events carry the business data.\n",
    "\n",
    "The code below operates a left outer join to list, within all the completed activity events, those that are part of an uncompleted process instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = spark.read.json(hdfsroot+\"/ibm-bai/bpmn-summaries-completed/\"+processAppId+\"/\"+processAppVersionId+\"/activity/*/*/*\").createOrReplaceTempView(\"activities\")\n",
    "\n",
    "activitiesOfUnfinishedProcesses = spark.sql(\"select p.startTime, a.name, a.timeStamp, a.data.aEmpRequisition121381434563922.* , p.processInstanceId from activities a LEFT OUTER JOIN unfinishedProcesses p ON a.processInstanceId=p.processInstanceId where p.processInstanceId is not NULL\")\n",
    "activitiesOfUnfinishedProcesses.show()\n",
    "\n",
    "activitiesOfUnfinishedProcesses.createOrReplaceTempView(\"activitiesOfUnfinishedProcesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now want to find the last activity of the unfinished processes. This is done by partitioning the events by the process instance identifier, then ordering them by the timestamp, and taking the first event. The result consists of the last activity event that was stored for each uncompleted process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinishedProcesses = spark.sql(\"SELECT distinct * FROM ( SELECT *, dense_rank() OVER (PARTITION BY processInstanceId ORDER BY timeStamp DESC) AS rank  FROM activitiesOfUnfinishedProcesses) vo WHERE rank = 1\");\n",
    "unfinishedProcesses.show()\n",
    "unfinishedProcesses.createOrReplaceTempView(\"unfinishedProcesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML model can now be used to predict the duration of the uncompleted processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"Department.string\", \"Department\")\n",
    "unfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"HiringManager.string\", \"HiringManager\")\n",
    "unfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"Location.string\", \"Location\")\n",
    "unfinishedProcesses = unfinishedProcesses.withColumnRenamed(\"EmploymentStatus.string\", \"EmploymentStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(unfinishedProcesses).select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You have seen the various type of events that are stored in Business Automation Insights for activities and processes, and how you can take benefit of them to predict the process duration, based on the business data tracked by a process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Emmanuel Tissandier is a Senior Technical Staff Member and architect in the Digital Business Automation team in the IBM France Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
